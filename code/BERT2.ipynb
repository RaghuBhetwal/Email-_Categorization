{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-28T12:43:39.274429Z","iopub.execute_input":"2023-06-28T12:43:39.274812Z","iopub.status.idle":"2023-06-28T12:43:39.285710Z","shell.execute_reply.started":"2023-06-28T12:43:39.274784Z","shell.execute_reply":"2023-06-28T12:43:39.284669Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/selected-data/selected_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:41.799629Z","iopub.execute_input":"2023-06-28T12:43:41.800122Z","iopub.status.idle":"2023-06-28T12:43:41.807865Z","shell.execute_reply.started":"2023-06-28T12:43:41.800082Z","shell.execute_reply":"2023-06-28T12:43:41.806740Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:43.597871Z","iopub.execute_input":"2023-06-28T12:43:43.598535Z","iopub.status.idle":"2023-06-28T12:43:43.604027Z","shell.execute_reply.started":"2023-06-28T12:43:43.598502Z","shell.execute_reply":"2023-06-28T12:43:43.602863Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/selected-data/selected_data.csv')\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:53.120459Z","iopub.execute_input":"2023-06-28T12:43:53.121163Z","iopub.status.idle":"2023-06-28T12:43:54.402373Z","shell.execute_reply.started":"2023-06-28T12:43:53.121122Z","shell.execute_reply":"2023-06-28T12:43:54.395125Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(29026, 6)"},"metadata":{}}]},{"cell_type":"code","source":"\n# Preprocess the text\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_length = 512","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:54.663421Z","iopub.execute_input":"2023-06-28T12:43:54.663764Z","iopub.status.idle":"2023-06-28T12:43:54.851738Z","shell.execute_reply.started":"2023-06-28T12:43:54.663735Z","shell.execute_reply":"2023-06-28T12:43:54.850776Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = str(text)\n    encoded_text = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    return encoded_text['input_ids'], encoded_text['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:56.625629Z","iopub.execute_input":"2023-06-28T12:43:56.626180Z","iopub.status.idle":"2023-06-28T12:43:56.633127Z","shell.execute_reply.started":"2023-06-28T12:43:56.626138Z","shell.execute_reply":"2023-06-28T12:43:56.632098Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nX = data['body']\ny = data['To']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_ids, X_train_mask = zip(*[preprocess_text(text) for text in X_train])\nX_test_ids, X_test_mask = zip(*[preprocess_text(text) for text in X_test])\n\nX_train_ids = torch.cat(X_train_ids, dim=0)\nX_train_mask = torch.cat(X_train_mask, dim=0)\nX_test_ids = torch.cat(X_test_ids, dim=0)\nX_test_mask = torch.cat(X_test_mask, dim=0)\n\n# Perform label encoding\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n## Convert numpy arrays to tensors\ny_train = torch.tensor(y_train_encoded).long()\ny_test = torch.tensor(y_test_encoded).long()\n\n# Create the train and test datasets\ntrain_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)\ntest_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)\n# Create the dataloaders\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:43:58.537665Z","iopub.execute_input":"2023-06-28T12:43:58.538046Z","iopub.status.idle":"2023-06-28T12:49:45.374942Z","shell.execute_reply.started":"2023-06-28T12:43:58.538016Z","shell.execute_reply":"2023-06-28T12:49:45.373960Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n# Create the model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set the optimizer and loss function\noptimizer = transformers.AdamW(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:49:45.377078Z","iopub.execute_input":"2023-06-28T12:49:45.377467Z","iopub.status.idle":"2023-06-28T12:49:53.616463Z","shell.execute_reply.started":"2023-06-28T12:49:45.377431Z","shell.execute_reply":"2023-06-28T12:49:53.615488Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc7aa3c38934a94ac265d16384bbefd"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Training loop\nepochs = 10\nhistory = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n\ntotal_steps = len(train_dataloader) * epochs\ncurrent_step = 0\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    train_acc = 0.0\n\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, attention_mask, labels = batch\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        _, preds = torch.max(logits, dim=1)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * input_ids.size(0)\n        train_acc += torch.sum(preds == labels).item()\n\n        current_step += 1\n        progress = float(current_step) / total_steps\n        print(f\"Epoch: {epoch + 1}/{epochs}, Step: {step + 1}/{len(train_dataloader)}, Progress: {progress:.2%}\", end=\"\\r\")\n\n    train_loss /= len(train_dataset)\n    train_acc /= len(train_dataset)\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n\n    # Evaluation on test set\n    model.eval()\n    test_loss = 0.0\n    test_acc = 0.0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, attention_mask, labels = batch\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n            _, preds = torch.max(logits, dim=1)\n\n            test_loss += loss.item() * input_ids.size(0)\n            test_acc += torch.sum(preds == labels).item()\n\n    test_loss /= len(test_dataset)\n    test_acc /= len(test_dataset)\n    history['test_loss'].append(test_loss)\n    history['test_acc'].append(test_acc)\n\n    print(f\"Epoch: {epoch + 1}/{epochs}, Step: {step + 1}/{len(train_dataloader)}, Progress: {progress:.2%}\")\n    print(f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.4f}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:49:53.618048Z","iopub.execute_input":"2023-06-28T12:49:53.618648Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 1/10, Step: 1452/1452, Progress: 10.00%\nTrain Loss: 0.2200 Train Acc: 0.9183\nTest Loss: 0.1009 Test Acc: 0.9612\n\nEpoch: 2/10, Step: 1452/1452, Progress: 20.00%\nTrain Loss: 0.0638 Train Acc: 0.9750\nTest Loss: 0.0670 Test Acc: 0.9693\n\nEpoch: 3/10, Step: 1452/1452, Progress: 30.00%\nTrain Loss: 0.0327 Train Acc: 0.9857\nTest Loss: 0.0669 Test Acc: 0.9776\n\nEpoch: 4/10, Step: 1452/1452, Progress: 40.00%\nTrain Loss: 0.0303 Train Acc: 0.9865\nTest Loss: 0.0808 Test Acc: 0.9687\n\nEpoch: 5/10, Step: 1452/1452, Progress: 50.00%\nTrain Loss: 0.0243 Train Acc: 0.9875\nTest Loss: 0.0643 Test Acc: 0.9802\n\nEpoch: 6/10, Step: 1452/1452, Progress: 60.00%\nTrain Loss: 0.0241 Train Acc: 0.9886\nTest Loss: 0.0599 Test Acc: 0.9811\n\nEpoch: 7/10, Step: 1452/1452, Progress: 70.00%\nTrain Loss: 0.0207 Train Acc: 0.9888\nTest Loss: 0.0740 Test Acc: 0.9738\n\nEpoch: 8/10, Step: 1365/1452, Progress: 79.40%\r","output_type":"stream"}]},{"cell_type":"code","source":"# CrossEntropyLoss\n# Adaptive Moment Estimation with Weight Decay\n# Saving the model\ntorch.save(model.state_dict(), 'saved_model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation loss\nplt.plot(history['train_loss'], label='Training Loss')\nplt.plot(history['test_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n\n# Plot training and validation accuracy\nplt.plot(history['train_acc'], label='Training Accuracy')\nplt.plot(history['test_acc'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate classification report\nmodel.eval()\ny_pred_labels = []\nwith torch.no_grad():\n    for batch in test_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, attention_mask, labels = batch\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        _, preds = torch.max(logits, dim=1)\n        y_pred_labels.extend(preds.cpu().numpy())\n\ny_pred_labels = label_encoder.inverse_transform(y_pred_labels)\nprint(classification_report(y_test, y_pred_labels))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Loading the saved model\nloaded_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\nloaded_model.load_state_dict(torch.load('saved_model.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n# Sample email\nsample_email = [\"I hope this email finds you well. I wanted to schedule a meeting to discuss the upcoming project. Please let me know your availability. Thank you.\"]\npreprocessed_sample = [preprocess_text(email) for email in sample_email]\n\n# Tokenize and pad the preprocessed sample email(s):\nsample_sequences = tokenizer.texts_to_sequences(preprocessed_sample)\nsample_padded = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n\n# Convert to tensors\nsample_input_ids = torch.tensor(sample_padded).to(device)\nsample_attention_mask = torch.tensor((sample_padded > 0), dtype=torch.long).to(device)\n\n# Predict the recipient(s) using the trained model:\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(sample_input_ids, attention_mask=sample_attention_mask)\n    logits = outputs.logits\n    predicted_classes = logits.argmax(dim=1).cpu().numpy()\n\npredicted_labels = label_encoder.inverse_transform(predicted_classes)\nprint(\"Predicted Recipient(s):\", predicted_labels)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body_2 = data.loc[1, 'body']\nbody_2 = data.loc[1, 'body']\nrecipient_2 = data.loc[1, 'To']\n\nprint(\"Body (2nd email):\", body_2)\nprint(\"Original Recipient (2nd email):\", recipient_2)\n\n# Preprocessing\npreprocessed_body_2 = preprocess_text(body_2)\n\n# Tokenize and pad the preprocessed body:\nbody_2_sequence = tokenizer.texts_to_sequences([preprocessed_body_2])\nbody_2_padded = pad_sequences(body_2_sequence, maxlen=max_length, padding='post')\n\n# Convert to tensors\nbody_2_input_ids = torch.tensor(body_2_padded).to(device)\nbody_2_attention_mask = torch.tensor((body_2_padded > 0), dtype=torch.long).to(device)\n\n# Predict the recipient using the trained model:\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(body_2_input_ids, attention_mask=body_2_attention_mask)\n    logits = outputs.logits\n    predicted_recipient_2 = logits.argmax(dim=1).cpu().numpy()\n\npredicted_label_2 = label_encoder.inverse_transform(predicted_recipient_2)\nprint(\"Predicted Recipient (2nd email):\", predicted_label_2)\n","metadata":{},"execution_count":null,"outputs":[]}]}